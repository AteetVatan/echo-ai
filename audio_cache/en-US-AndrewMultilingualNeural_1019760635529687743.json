{
    "text": "I have a comprehensive suite of NLP and text processing capabilities that I've built and integrated across my projects. Here's what I can do:\n\nNamed Entity Recognition NER: \nI use distilbert-base-multilingual-cased-ner-hrl from Hugging Face for multilingual entity extraction across languages.\n\nLanguage Detection: \nFastText LID model for accurate language identification.\n\nMachine Translation: \nI use argostranslate for title translation and NLLB facebook/nllb-200-distilled-600M for more advanced multilingual translation tasks.\n\nText Extraction: \nI've built a robust 4-method ensemble pipeline: \n1. Trafilatura primary \n2. readability-lxml \n3. jusText \n4. BoilerPy3 \nThis ensures reliable article extraction even from poorly structured HTML.\n\nDeduplication: \nSHA-256 for exact duplicate detection + MinHash LSH for near-duplicate identification.\n\nText Embeddings: \nSentence-transformers with all-MiniLM-L6-v2 model for semantic vector representations.\n\nTopic Classification: \nONNX IPTC classifier for categorizing news content.\n\nSummarization: \nTwo-stage pipeline: local DistilBART CNN for initial summarization, then LLM-powered synthesis using Together AI's Llama 3.2 3B Instruct Turbo for cluster-level summaries.\n\nFull-Text Search: \nGerman language support with advanced indexing.\n\nAdditional NLP Tools: \nSpaCy 3.8.7 for industrial-strength NLP pipelines \nNLTK 3.9.1 for traditional NLP tasks \nTextBlob for text processing \njieba3k for Chinese text segmentation \nBlingFire for sentence segmentation\n\nThese capabilities power my MASX-GeoSignal system which processes 10,000+ multilingual news URLs daily through a fully automated pipeline, from raw HTML ingestion to structured intelligence output with geo-entity resolution and hotspot scoring.",
    "voice_id": "en-US-AndrewMultilingualNeural",
    "file_path": "audio_cache\\en-US-AndrewMultilingualNeural_1019760635529687743.mp3",
    "created_at": "2026-02-18T14:18:38.371369"
}